{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load private access logs.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the dataset CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by normalizing numerical values and extracting text features.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Processed features, vectorized text, and time series data.\n",
    "    \"\"\"\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['file_size', 'access_duration']] = scaler.fit_transform(df[['file_size', 'access_duration']])\n",
    "    \n",
    "    # TF-IDF Vectorization for textual features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    text_features = vectorizer.fit_transform(df['file_type'])\n",
    "    \n",
    "    # Doc2Vec for additional embeddings\n",
    "    tagged_data = [TaggedDocument(words=row.split(), tags=[i]) for i, row in enumerate(df['file_type'])]\n",
    "    doc2vec_model = Doc2Vec(tagged_data, vector_size=50, window=2, min_count=1, workers=4)\n",
    "    doc2vec_embeddings = [doc2vec_model.infer_vector(row.split()) for row in df['file_type']]\n",
    "    \n",
    "    # Time series extraction\n",
    "    time_series_data = df[['timestamp', 'file_size']].set_index('timestamp').resample('H').sum()\n",
    "    \n",
    "    return df, text_features, doc2vec_embeddings, time_series_data\n",
    "\n",
    "# Train Isolation Forest for anomaly detection\n",
    "def train_isolation_forest(features):\n",
    "    \"\"\"\n",
    "    Train an Isolation Forest model for anomaly detection.\n",
    "    \n",
    "    Parameters:\n",
    "        features: Feature set to train the model.\n",
    "    \n",
    "    Returns:\n",
    "        IsolationForest: Trained model.\n",
    "    \"\"\"\n",
    "    model = IsolationForest(contamination=0.01, random_state=42)\n",
    "    model.fit(features)\n",
    "    return model\n",
    "\n",
    "# Train ARIMA for time series anomaly detection\n",
    "def train_arima(time_series_data):\n",
    "    \"\"\"\n",
    "    Train an ARIMA model for time series anomaly detection.\n",
    "    \n",
    "    Parameters:\n",
    "        time_series_data (pd.DataFrame): Time series data.\n",
    "    \n",
    "    Returns:\n",
    "        ARIMA: Trained ARIMA model.\n",
    "    \"\"\"\n",
    "    model = ARIMA(time_series_data, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    return model_fit\n",
    "\n",
    "# Deploy model using Kubeflow (placeholder function)\n",
    "def deploy_model(model, model_name):\n",
    "    \"\"\"\n",
    "    Deploy model using Kubeflow.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained model.\n",
    "        model_name (str): Name for deployment.\n",
    "    \"\"\"\n",
    "    joblib.dump(model, f'{model_name}.pkl')\n",
    "    print(f\"{model_name} model deployed using Kubeflow.\")\n",
    "\n",
    "# Example execution\n",
    "file_path = 'private_access_logs.csv'  # Update with actual dataset\n",
    "\n",
    "# Load and preprocess data\n",
    "df = load_data(file_path)\n",
    "df, text_features, doc2vec_embeddings, time_series_data = preprocess_data(df)\n",
    "\n",
    "# Train models\n",
    "iso_forest_model = train_isolation_forest(text_features.toarray())\n",
    "arima_model = train_arima(time_series_data)\n",
    "\n",
    "# Deploy models\n",
    "deploy_model(iso_forest_model, 'isolation_forest')\n",
    "deploy_model(arima_model, 'arima_model')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
